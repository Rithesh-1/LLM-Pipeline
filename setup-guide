### **Prerequisites**

Ensure you have the following tools installed on your system:
*   Git
*   Python 3.11
*   Poetry
*   Docker

---

### **Step 1: Initial Project Setup**

First, set up the project, install dependencies, and configure your API keys.

1.  **Clone the repository and enter the directory:**
    ```bash
    git clone <your-github-repository-url>
    cd <repository-name>
```

2.  **Install dependencies:**
    ```bash
    poetry install --no-root
    ```
    
    **Optional**: To use the training pipeline, install training dependencies:
    ```bash
    poetry install --no-root --extras training
    ```

3.  **Create and configure your environment file:**
    Copy the example file:
    ```bash
    cp .env.example .env
    ```
    Now, open the `.env` file and fill in the following variables:
    * `OPENAI_API_KEY`: Your key from OpenAI (optional, for dataset generation with OpenAI).
    * `GOOGLE_API_KEY`: Your key from Google AI Studio (optional, for dataset generation with Gemini Flash).
    * `HUGGINGFACE_ACCESS_TOKEN`: Your token from Hugging Face (required for saving models).

    **Note**: You need either `OPENAI_API_KEY` or `GOOGLE_API_KEY` for dataset generation. The default configuration uses Gemini 1.5-flash.

    **Switching between OpenAI and Gemini**: To use OpenAI instead of Gemini, modify the configuration files:
    * For instruction datasets: Use `configs/generate_instruct_datasets_openai.yaml`
    * For preference datasets: Use `configs/generate_preference_datasets_openai.yaml`
    * Or change `llm_provider: "gemini"` to `llm_provider: "openai"` in the existing config files.

---

### **Step 2: Start Local Infrastructure**

Start the local MongoDB and Qdrant databases using Docker. These will run in the background.
```bash
docker-compose up -d
```

---

### **Step 3: Start the MLflow UI**

Open a **new terminal** and start the MLflow server. This will allow you to track the progress of your data processing and training pipelines.
```bash
mlflow ui
```
Keep this terminal running. You can view the MLflow dashboard in your browser at `http://127.0.0.1:5000`.

---

### **Step 4: Run the Data and Training Pipelines**

Execute the following commands one by one in your original terminal. Each command runs a specific stage of the ML pipeline, and you can monitor their progress in the MLflow UI.

1.  **Crawl Web Data:**
    ```bash
    poetry run python -m pipelines.run_mlflow --pipeline digital_data_etl --config-path configs/digital_data_etl_maxime_labonne.yaml
    ```

2.  **Process Data and Load into Databases:**
    ```bash
    poetry run python -m pipelines.run_mlflow --pipeline feature_engineering --config-path configs/end_to_end_data.yaml
    ```

3.  **Generate Instruction Dataset** (Uses Gemini Flash by default, OpenAI also supported):
    ```bash
    poetry run python -m pipelines.run_mlflow --pipeline generate_instruct_datasets --config-path configs/generate_instruct_datasets.yaml
    ```

4.  **Generate Preference Dataset** (Uses Gemini Flash by default, OpenAI also supported):
    ```bash
    poetry run python -m pipelines.run_mlflow --pipeline generate_preference_datasets --config-path configs/generate_preference_datasets.yaml
    ```

5.  **Fine-Tune the Model (SFT):**
    ```bash
    poetry run python -m llm_engineering.model.finetuning.finetune --finetuning_type sft --num_train_epochs 3 --learning_rate 0.0003 --is_dummy False
    ```

6.  **Fine-Tune the Model (DPO):**
    ```bash
    mlflow run . -e training -P finetuning_type=dpo --no-conda
    ```

---

### **Step 5: Serve the RAG Inference API**

After the pipelines are complete, you can serve your fine-tuned model with the RAG API using BentoML.

1.  **Start the BentoML Server:**
    ```bash
    bentoml serve
    ```
    Your API is now live and accessible at `http://127.0.0.1:3000`.

2.  **Query the API:**
    Open a **new terminal** and use `curl` to send a question to your RAG model.
    ```bash
    curl -X POST -H "Content-Type: application/json" -d '"What is Retrieval-Augmented Generation?"' http://127.0.0.1:3000/rag_query
    ```

You now have a fully operational, end-to-end RAG system running entirely on your local machine.
