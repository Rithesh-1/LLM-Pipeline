name: llm-engineers-handbook

entry_points:
  digital_data_etl:
    parameters:
      config_path: {type: str, default: "configs/digital_data_etl_maxime_labonne.yaml"}
    command: "python -m pipelines.run_mlflow --pipeline digital_data_etl --config-path {config_path}"
  
  feature_engineering:
    parameters:
      config_path: {type: str, default: "configs/end_to_end_data.yaml"}
    command: "python -m pipelines.run_mlflow --pipeline feature_engineering --config-path {config_path}"

  generate_instruct_datasets:
    parameters:
      config_path: {type: str, default: "configs/generate_instruct_datasets.yaml"}
    command: "python -m pipelines.run_mlflow --pipeline generate_instruct_datasets --config-path {config_path}"

  generate_preference_datasets:
    parameters:
      config_path: {type: str, default: "configs/generate_preference_datasets.yaml"}
    command: "python -m pipelines.run_mlflow --pipeline generate_preference_datasets --config-path {config_path}"

  training:
    parameters:
      finetuning_type: {type: str, default: "sft"}
      num_train_epochs: {type: int, default: 3}
      learning_rate: {type: float, default: 0.0003}
      is_dummy: {type: bool, default: False}
    command: "python -m llm_engineering.model.finetuning.finetune --finetuning_type {finetuning_type} --num_train_epochs {num_train_epochs} --learning_rate {learning_rate} --is_dummy {is_dummy}"
  
  evaluation:
    parameters:
      config_path: {type: str, default: "configs/evaluating.yaml"}
    command: "python -m pipelines.run_mlflow --pipeline evaluation --config-path {config_path}"

